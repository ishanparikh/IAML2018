{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introductory applied machine learning (INFR10069)\n",
    "# Assignment 3 (Part A): Object Recognition [75%]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Important Instructions\n",
    "\n",
    "**It is important that you follow the instructions below to the letter - we will not be responsible for incorrect marking due to non-standard practices.**\n",
    "\n",
    "1. <font color='red'>We have split Assignment 3 into two parts to make it easier for you to work on them separately and for the markers to give you feedback. This is part A of Assignment 3 - Part B is the Mini-Challenge. Both Assignments together are still worth 50% of CourseWork 2. **Remember to submit both notebooks (you can submit them separately).**</font>\n",
    "\n",
    "1. You *MUST* have your environment set up as in the [README](https://github.com/michael-camilleri/IAML2018) and you *must activate this environment before running this notebook*:\n",
    "```\n",
    "source activate py3iaml\n",
    "cd [DIRECTORY CONTAINING GIT REPOSITORY]\n",
    "jupyter notebook\n",
    "# Navigate to this file\n",
    "```\n",
    "\n",
    "1. Read the instructions carefully, especially where asked to name variables with a specific name. Wherever you are required to produce code you should use code cells, otherwise you should use markdown cells to report results and explain answers. In most cases we indicate the nature of answer we are expecting (code/text), and also provide the code/markdown cell where to put it\n",
    "\n",
    "1. There are some questions which are **specific to those taking the Level-11 version** of the course (INFR11182 and INFR11152). These are clearly marked with the words **(LEVEL 11)** and must be completed by those taking the Level 11 course. Those on the Level 10 version (INFR10069) may (and are advised to) attempt such questions but this will not affect their mark in any way, nor will they get feedback on them.\n",
    "\n",
    "1. The .csv files that you will be using are located at `./datasets` (i.e. use the `datasets` directory **adjacent** to this file).\n",
    "\n",
    "1. Keep your answers brief and concise. Most written questions can be answered with 2-3 lines of explanation: **in questions where this is specified, you will be penalised if you go over.**\n",
    "\n",
    "1. Make sure to distinguish between **attributes** (columns of the data) and **features** (typically referring only to the independent variables).\n",
    "\n",
    "1. Make sure to show **all** your code/working. \n",
    "\n",
    "1. Write readable code. While we do not expect you to follow [PEP8](https://www.python.org/dev/peps/pep-0008/) to the letter, the code should be adequately understandable, with plots/visualisations correctly labelled. **Do** use inline comments when doing something non-standard. When asked to present numerical values, make sure to represent real numbers in the appropriate precision to exemplify your answer. Marks *WILL* be deducted if the marker cannot understand your logic/results.\n",
    "\n",
    "1. **Collaboration:** You may discuss the assignment with your colleagues, provided that the writing that you submit is entirely your own. That is, you must NOT borrow actual text or code from other students. We ask that you provide a list of the people who you've had discussions with (if any). Please refer to the [Academic Misconduct](http://web.inf.ed.ac.uk/infweb/admin/policies/academic-misconduct) page for what consistutes a breach of the above.\n",
    "\n",
    "### SUBMISSION Mechanics\n",
    "\n",
    "**IMPORTANT:** You must submit this assignment by **Thursday 15/11/2018 at 16:00**. \n",
    "\n",
    "**Late submissions:** The policy stated in the School of Informatics is that normally you will not be allowed to submit coursework late. See the [ITO webpage](http://web.inf.ed.ac.uk/infweb/student-services/ito/admin/coursework-projects/late-coursework-extension-requests) for exceptions to this, e.g. in case of serious medical illness or serious personal problems.\n",
    "\n",
    "**Resubmission:** If you submit your file(s) again, the previous submission is **overwritten**. We will mark the version that is in the submission folder at the deadline.\n",
    "\n",
    "**N.B.**: This Assignment requires submitting **two files (electronically as described below)**:\n",
    " 1. This Jupyter Notebook for Part A, *and*\n",
    " 1. The Jupyter Notebook for Part B\n",
    " \n",
    "All submissions happen electronically. To submit:\n",
    "\n",
    "1. Fill out this notebook (as well as Part B), making sure to:\n",
    "   1. save it with **all code/text and visualisations**: markers are NOT expected to run any cells,\n",
    "   1. keep the name of the file **UNCHANGED**, *and*\n",
    "   1. **keeping the same structure**: retain the questions, and avoid adding unnecessary cells unless absolutely necessary, as this makes the job harder for the markers.\n",
    "\n",
    "1. Submit it using the `submit` functionality. To do this, you must be on a DICE environment. Open a Terminal, and:\n",
    "   1. **On-Campus Students**: navigate to the location of this notebook and execute the following command:\n",
    "   \n",
    "      ```submit iaml cw2 03_A_ObjectRecognition.ipynb 03_B_MiniChallenge.ipynb```\n",
    "      \n",
    "   1. **Distance Learners:** These instructions also apply to those students who work on their own computer. First you need to copy your work onto DICE (so that you can use the `submit` command). For this, you can use `scp` or `rsync` (you may need to install these yourself). You can copy files to `student.ssh.inf.ed.ac.uk`, then ssh into it in order to submit. The following is an example. Replace entries in `[square brackets]` with your specific details: i.e. if your student number is for example s1234567, then `[YOUR USERNAME]` becomes `s1234567`.\n",
    "   \n",
    "    ```\n",
    "    scp -r [FULL PATH TO 03_A_ObjectRecognition.ipynb] [YOUR USERNAME]@student.ssh.inf.ed.ac.uk:03_A_ObjectRecognition.ipynb\n",
    "    scp -r [FULL PATH TO 03_B_MiniChallenge.ipynb] [YOUR USERNAME]@student.ssh.inf.ed.ac.uk:03_B_MiniChallenge.ipynb\n",
    "    ssh [YOUR USERNAME]@student.ssh.inf.ed.ac.uk\n",
    "    ssh student.login\n",
    "    submit iaml cw2 03_A_ObjectRecognition.ipynb 03_B_MiniChallenge.ipynb\n",
    "    ```\n",
    "    \n",
    "   What actually happens in the background is that your file is placed in a folder available to markers. If you submit a file with the same name into the same location, **it will *overwrite* your previous submission**. You should receive an automatic email confirmation after submission.\n",
    "  \n",
    "\n",
    "\n",
    "### Marking Breakdown\n",
    "\n",
    "The Level 10 and Level 11 points are marked out of different totals, however these are all normalised to 100%. Note that Part A (this notebook) is worth 75% of the total Mark for Assignment 3, while Part B (Mini-Challenge) is worth 25%: *keep this breakdown in mind when planning your work, especially for Part B*.\n",
    "\n",
    "**70-100%** results/answer correct plus extra achievement at understanding or analysis of results. Clear explanations, evidence of creative or deeper thought will contribute to a higher grade.\n",
    "\n",
    "**60-69%** results/answer correct or nearly correct and well explained.\n",
    "\n",
    "**50-59%** results/answer in right direction but significant errors.\n",
    "\n",
    "**40-49%** some evidence that the student has gained some understanding, but not answered the questions\n",
    "properly.\n",
    "\n",
    "**0-39%** serious error or slack work.\n",
    "\n",
    "Note that while this is not a programming assignment, in questions which involve visualisation of results and/or long cold snippets, some marks may be deducted if the code is not adequately readable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n",
    "\n",
    "Use the cell below to include any imports you deem necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-03T15:51:53.710629Z",
     "start_time": "2018-11-03T15:51:52.138321Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ishan/anaconda3/envs/py3iaml/lib/python3.7/site-packages/sklearn/utils/__init__.py:4: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working\n",
      "  from collections import Sequence\n"
     ]
    }
   ],
   "source": [
    "# Nice Formatting within Jupyter Notebook\n",
    "%matplotlib inline\n",
    "from IPython.display import display # Allows multiple displays from a single code-cell\n",
    "\n",
    "# System functionality\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "\n",
    "# Import Here any Additional modules you use. To import utilities we provide, use something like:\n",
    "#   from utils.plotter import plot_hinton\n",
    "\n",
    "# Your Code goes here:\n",
    "\n",
    "from utils.plotter import *\n",
    "import os\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import graphviz\n",
    "from sklearn import preprocessing\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import confusion_matrix, log_loss\n",
    "from pandas.api.types import CategoricalDtype\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_predict\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, r2_score\n",
    "from sklearn import linear_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description of the dataset\n",
    "In this assignment our goal is to recognize objects in images of realistic scenes. There are 19 different classes of object e.g. person, dog, cat, car, etc. The dataset derives from several thousands photographs harvested from the web. Each object of a relevant class has been manually annotated with a bounding box. Images can contain none, one or multiple objects of each class. We have prepared a [website](http://www.inf.ed.ac.uk/teaching/courses/iaml/2014/assts/asst3/images.html) where you can view the images.\n",
    "\n",
    "We are going to detect whether images contain a person or not - a binary classification problem. To save you time and to make the problem manageable with limited computational resources, we have preprocessed the dataset. We will use the [Bag of Visual Words](https://en.wikipedia.org/wiki/Bag-of-words_model_in_computer_vision) representation. Each column of the dataset (which is not a label), refers to a 'visual word'. Each image is represented by a 500 dimensional vector that contains the normalized count for each of 500 different visual words present in the respective image (a similar representation is used for the spambase dataset, just for real words). *Note that the normalisation procedure involves dividing the count vector by the total number of visual words in the image, i.e. the normalized counts sum to 1 for each image*. See the Appendix at the bottom of the notebook for more information. The image data is thus a $N \\times 500$ dimensional matrix where `N` is the number of images.\n",
    "\n",
    "The full dataset has 520 attributes (dimensions). The first attribute (`imgId`) contains the image ID which allows you to associate a data point with an actual image. The next 500 attributes (`dim1`, ..., `dim500`) are a normalized count vector for each visual word: these are the `features` of the data. The last 19 attributes, which follow the pattern `is_[class]` are the class labels -- here 1 means the class is present in the image. In most of the experiments (unless explicitly noted otherwise) you will only need the `is_person` attribute and the 500 dimensional feature vector. **Do not use the additional class indicator attributes as features** unless explicitly told to do so. \n",
    "\n",
    "**Important**: *Throughout the assignment you will be given various versions of the dataset that are relevant\n",
    "to a particular question. Please be careful to use the correct version of the dataset when instructed to do so.\n",
    "If you use the wrong version of the dataset by mistake no marks will be awarded.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Exploration of the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='question1_1'></a>\n",
    "### ========== Question 1.1 --- [12 marks] ==========\n",
    "\n",
    "We will first get a feel for the data. *IMPORTANT: Show all your code!*\n",
    "\n",
    "&nbsp;&nbsp;**(a)** [Code] Load the training dataset `Images_A_Train.csv` into a pandas dataframe, keeping only the Visual Features and the `is_person` column. <br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;*Hint: You may wish to first have a look at the column names*<br>\n",
    "&nbsp;&nbsp;**(b)** [Code] Using suitable pandas methods, summarise the key properties of the data, *and*<br>\n",
    "&nbsp;&nbsp;**(c)** [Text] comment on your observations from ***(b)*** (dimensionality, data ranges, anything out of the ordinary)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-03T15:51:54.156153Z",
     "start_time": "2018-11-03T15:51:53.713953Z"
    }
   },
   "outputs": [],
   "source": [
    "# (a) # Your Code goes here:\n",
    "datapath = os.path.join(os.getcwd(),'datasets','Images_A_Train.csv')\n",
    "trnData = pd.read_csv(datapath,delimiter=',')\n",
    "# data = data.drop(axis=1)\n",
    "classLabels = trnData.columns[-19:]\n",
    "classLabels = classLabels.drop('is_person')\n",
    "# display(classLabels)\n",
    "trnData.drop(axis=1, inplace=True, columns=classLabels)\n",
    "trnData.drop(axis=1, inplace=True, columns='imgId')\n",
    "# print(len(classLabels))\n",
    "# display(data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-03T15:51:56.048516Z",
     "start_time": "2018-11-03T15:51:54.159331Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Looking at first 5 rows:'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dim1</th>\n",
       "      <th>dim2</th>\n",
       "      <th>dim3</th>\n",
       "      <th>dim4</th>\n",
       "      <th>dim5</th>\n",
       "      <th>dim6</th>\n",
       "      <th>dim7</th>\n",
       "      <th>dim8</th>\n",
       "      <th>dim9</th>\n",
       "      <th>dim10</th>\n",
       "      <th>...</th>\n",
       "      <th>dim492</th>\n",
       "      <th>dim493</th>\n",
       "      <th>dim494</th>\n",
       "      <th>dim495</th>\n",
       "      <th>dim496</th>\n",
       "      <th>dim497</th>\n",
       "      <th>dim498</th>\n",
       "      <th>dim499</th>\n",
       "      <th>dim500</th>\n",
       "      <th>is_person</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.002232</td>\n",
       "      <td>0.000558</td>\n",
       "      <td>0.002790</td>\n",
       "      <td>0.000837</td>\n",
       "      <td>0.001674</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.001395</td>\n",
       "      <td>0.002232</td>\n",
       "      <td>0.003627</td>\n",
       "      <td>0.006138</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000558</td>\n",
       "      <td>0.005301</td>\n",
       "      <td>0.001116</td>\n",
       "      <td>0.004185</td>\n",
       "      <td>0.000837</td>\n",
       "      <td>0.006975</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.001674</td>\n",
       "      <td>0.000558</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.001563</td>\n",
       "      <td>0.000391</td>\n",
       "      <td>0.007422</td>\n",
       "      <td>0.003516</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.005078</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.002344</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.008203</td>\n",
       "      <td>0.001172</td>\n",
       "      <td>0.007422</td>\n",
       "      <td>0.004297</td>\n",
       "      <td>0.001563</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003125</td>\n",
       "      <td>0.004687</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000521</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001042</td>\n",
       "      <td>0.001563</td>\n",
       "      <td>0.005729</td>\n",
       "      <td>0.000521</td>\n",
       "      <td>0.002083</td>\n",
       "      <td>0.003646</td>\n",
       "      <td>0.005208</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000521</td>\n",
       "      <td>0.000521</td>\n",
       "      <td>0.002083</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000521</td>\n",
       "      <td>0.003646</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.002976</td>\n",
       "      <td>0.002232</td>\n",
       "      <td>0.004464</td>\n",
       "      <td>0.000372</td>\n",
       "      <td>0.000372</td>\n",
       "      <td>0.002232</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003720</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002232</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.015253</td>\n",
       "      <td>0.000744</td>\n",
       "      <td>0.001488</td>\n",
       "      <td>0.000744</td>\n",
       "      <td>0.000372</td>\n",
       "      <td>0.001860</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001860</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.001359</td>\n",
       "      <td>0.000340</td>\n",
       "      <td>0.001359</td>\n",
       "      <td>0.000340</td>\n",
       "      <td>0.001359</td>\n",
       "      <td>0.002038</td>\n",
       "      <td>0.002378</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003397</td>\n",
       "      <td>0.003397</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000679</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001359</td>\n",
       "      <td>0.001019</td>\n",
       "      <td>0.003736</td>\n",
       "      <td>0.008152</td>\n",
       "      <td>0.003736</td>\n",
       "      <td>0.000679</td>\n",
       "      <td>0.001698</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 501 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       dim1      dim2      dim3      dim4      dim5      dim6      dim7  \\\n",
       "0  0.002232  0.000558  0.002790  0.000837  0.001674  0.001953  0.001395   \n",
       "1  0.001563  0.000391  0.007422  0.003516  0.003906  0.005078  0.001953   \n",
       "2  0.000521  0.000000  0.000000  0.001042  0.001563  0.005729  0.000521   \n",
       "3  0.002976  0.002232  0.004464  0.000372  0.000372  0.002232  0.000000   \n",
       "4  0.001359  0.000340  0.001359  0.000340  0.001359  0.002038  0.002378   \n",
       "\n",
       "       dim8      dim9     dim10    ...        dim492    dim493    dim494  \\\n",
       "0  0.002232  0.003627  0.006138    ...      0.000558  0.005301  0.001116   \n",
       "1  0.002344  0.001953  0.001953    ...      0.000000  0.008203  0.001172   \n",
       "2  0.002083  0.003646  0.005208    ...      0.000000  0.000521  0.000521   \n",
       "3  0.003720  0.000000  0.002232    ...      0.000000  0.015253  0.000744   \n",
       "4  0.000000  0.003397  0.003397    ...      0.000679  0.000000  0.001359   \n",
       "\n",
       "     dim495    dim496    dim497    dim498    dim499    dim500  is_person  \n",
       "0  0.004185  0.000837  0.006975  0.001953  0.001674  0.000558          1  \n",
       "1  0.007422  0.004297  0.001563  0.000000  0.003125  0.004687          0  \n",
       "2  0.002083  0.000000  0.000000  0.000521  0.003646  0.000000          0  \n",
       "3  0.001488  0.000744  0.000372  0.001860  0.000000  0.001860          1  \n",
       "4  0.001019  0.003736  0.008152  0.003736  0.000679  0.001698          0  \n",
       "\n",
       "[5 rows x 501 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Basic stats of datasets:'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dim1</th>\n",
       "      <th>dim2</th>\n",
       "      <th>dim3</th>\n",
       "      <th>dim4</th>\n",
       "      <th>dim5</th>\n",
       "      <th>dim6</th>\n",
       "      <th>dim7</th>\n",
       "      <th>dim8</th>\n",
       "      <th>dim9</th>\n",
       "      <th>dim10</th>\n",
       "      <th>...</th>\n",
       "      <th>dim492</th>\n",
       "      <th>dim493</th>\n",
       "      <th>dim494</th>\n",
       "      <th>dim495</th>\n",
       "      <th>dim496</th>\n",
       "      <th>dim497</th>\n",
       "      <th>dim498</th>\n",
       "      <th>dim499</th>\n",
       "      <th>dim500</th>\n",
       "      <th>is_person</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2093.000000</td>\n",
       "      <td>2093.000000</td>\n",
       "      <td>2093.000000</td>\n",
       "      <td>2093.000000</td>\n",
       "      <td>2093.000000</td>\n",
       "      <td>2093.000000</td>\n",
       "      <td>2093.000000</td>\n",
       "      <td>2093.000000</td>\n",
       "      <td>2093.000000</td>\n",
       "      <td>2093.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>2093.000000</td>\n",
       "      <td>2093.000000</td>\n",
       "      <td>2093.000000</td>\n",
       "      <td>2093.000000</td>\n",
       "      <td>2093.000000</td>\n",
       "      <td>2093.000000</td>\n",
       "      <td>2093.000000</td>\n",
       "      <td>2093.000000</td>\n",
       "      <td>2093.000000</td>\n",
       "      <td>2093.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.001751</td>\n",
       "      <td>0.000756</td>\n",
       "      <td>0.004317</td>\n",
       "      <td>0.001853</td>\n",
       "      <td>0.002272</td>\n",
       "      <td>0.002220</td>\n",
       "      <td>0.001019</td>\n",
       "      <td>0.002446</td>\n",
       "      <td>0.002088</td>\n",
       "      <td>0.002746</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000563</td>\n",
       "      <td>0.004586</td>\n",
       "      <td>0.001541</td>\n",
       "      <td>0.003642</td>\n",
       "      <td>0.002200</td>\n",
       "      <td>0.002928</td>\n",
       "      <td>0.002173</td>\n",
       "      <td>0.002485</td>\n",
       "      <td>0.002978</td>\n",
       "      <td>0.449116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.001193</td>\n",
       "      <td>0.001406</td>\n",
       "      <td>0.003693</td>\n",
       "      <td>0.001340</td>\n",
       "      <td>0.001598</td>\n",
       "      <td>0.001741</td>\n",
       "      <td>0.000873</td>\n",
       "      <td>0.002910</td>\n",
       "      <td>0.001971</td>\n",
       "      <td>0.002328</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001203</td>\n",
       "      <td>0.005825</td>\n",
       "      <td>0.001192</td>\n",
       "      <td>0.002657</td>\n",
       "      <td>0.001664</td>\n",
       "      <td>0.003254</td>\n",
       "      <td>0.001418</td>\n",
       "      <td>0.001997</td>\n",
       "      <td>0.002765</td>\n",
       "      <td>0.497523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000833</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001359</td>\n",
       "      <td>0.000756</td>\n",
       "      <td>0.001116</td>\n",
       "      <td>0.001019</td>\n",
       "      <td>0.000340</td>\n",
       "      <td>0.000625</td>\n",
       "      <td>0.000744</td>\n",
       "      <td>0.001116</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000679</td>\n",
       "      <td>0.000679</td>\n",
       "      <td>0.001698</td>\n",
       "      <td>0.001019</td>\n",
       "      <td>0.000744</td>\n",
       "      <td>0.001116</td>\n",
       "      <td>0.001019</td>\n",
       "      <td>0.001019</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.001563</td>\n",
       "      <td>0.000340</td>\n",
       "      <td>0.003397</td>\n",
       "      <td>0.001698</td>\n",
       "      <td>0.002038</td>\n",
       "      <td>0.001860</td>\n",
       "      <td>0.000744</td>\n",
       "      <td>0.001563</td>\n",
       "      <td>0.001698</td>\n",
       "      <td>0.002056</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002717</td>\n",
       "      <td>0.001359</td>\n",
       "      <td>0.003057</td>\n",
       "      <td>0.001860</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.002038</td>\n",
       "      <td>0.002038</td>\n",
       "      <td>0.002232</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.002378</td>\n",
       "      <td>0.000893</td>\n",
       "      <td>0.006324</td>\n",
       "      <td>0.002717</td>\n",
       "      <td>0.003057</td>\n",
       "      <td>0.003057</td>\n",
       "      <td>0.001488</td>\n",
       "      <td>0.003397</td>\n",
       "      <td>0.002717</td>\n",
       "      <td>0.003736</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000679</td>\n",
       "      <td>0.006454</td>\n",
       "      <td>0.002232</td>\n",
       "      <td>0.004808</td>\n",
       "      <td>0.003057</td>\n",
       "      <td>0.003780</td>\n",
       "      <td>0.002976</td>\n",
       "      <td>0.003397</td>\n",
       "      <td>0.004076</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.009851</td>\n",
       "      <td>0.016644</td>\n",
       "      <td>0.027514</td>\n",
       "      <td>0.010789</td>\n",
       "      <td>0.010417</td>\n",
       "      <td>0.021739</td>\n",
       "      <td>0.005774</td>\n",
       "      <td>0.029830</td>\n",
       "      <td>0.028372</td>\n",
       "      <td>0.020380</td>\n",
       "      <td>...</td>\n",
       "      <td>0.021739</td>\n",
       "      <td>0.053329</td>\n",
       "      <td>0.010234</td>\n",
       "      <td>0.024457</td>\n",
       "      <td>0.011719</td>\n",
       "      <td>0.028125</td>\n",
       "      <td>0.008492</td>\n",
       "      <td>0.014509</td>\n",
       "      <td>0.028533</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 501 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              dim1         dim2         dim3         dim4         dim5  \\\n",
       "count  2093.000000  2093.000000  2093.000000  2093.000000  2093.000000   \n",
       "mean      0.001751     0.000756     0.004317     0.001853     0.002272   \n",
       "std       0.001193     0.001406     0.003693     0.001340     0.001598   \n",
       "min       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "25%       0.000833     0.000000     0.001359     0.000756     0.001116   \n",
       "50%       0.001563     0.000340     0.003397     0.001698     0.002038   \n",
       "75%       0.002378     0.000893     0.006324     0.002717     0.003057   \n",
       "max       0.009851     0.016644     0.027514     0.010789     0.010417   \n",
       "\n",
       "              dim6         dim7         dim8         dim9        dim10  \\\n",
       "count  2093.000000  2093.000000  2093.000000  2093.000000  2093.000000   \n",
       "mean      0.002220     0.001019     0.002446     0.002088     0.002746   \n",
       "std       0.001741     0.000873     0.002910     0.001971     0.002328   \n",
       "min       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "25%       0.001019     0.000340     0.000625     0.000744     0.001116   \n",
       "50%       0.001860     0.000744     0.001563     0.001698     0.002056   \n",
       "75%       0.003057     0.001488     0.003397     0.002717     0.003736   \n",
       "max       0.021739     0.005774     0.029830     0.028372     0.020380   \n",
       "\n",
       "          ...            dim492       dim493       dim494       dim495  \\\n",
       "count     ...       2093.000000  2093.000000  2093.000000  2093.000000   \n",
       "mean      ...          0.000563     0.004586     0.001541     0.003642   \n",
       "std       ...          0.001203     0.005825     0.001192     0.002657   \n",
       "min       ...          0.000000     0.000000     0.000000     0.000000   \n",
       "25%       ...          0.000000     0.000679     0.000679     0.001698   \n",
       "50%       ...          0.000000     0.002717     0.001359     0.003057   \n",
       "75%       ...          0.000679     0.006454     0.002232     0.004808   \n",
       "max       ...          0.021739     0.053329     0.010234     0.024457   \n",
       "\n",
       "            dim496       dim497       dim498       dim499       dim500  \\\n",
       "count  2093.000000  2093.000000  2093.000000  2093.000000  2093.000000   \n",
       "mean      0.002200     0.002928     0.002173     0.002485     0.002978   \n",
       "std       0.001664     0.003254     0.001418     0.001997     0.002765   \n",
       "min       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "25%       0.001019     0.000744     0.001116     0.001019     0.001019   \n",
       "50%       0.001860     0.001953     0.002038     0.002038     0.002232   \n",
       "75%       0.003057     0.003780     0.002976     0.003397     0.004076   \n",
       "max       0.011719     0.028125     0.008492     0.014509     0.028533   \n",
       "\n",
       "         is_person  \n",
       "count  2093.000000  \n",
       "mean      0.449116  \n",
       "std       0.497523  \n",
       "min       0.000000  \n",
       "25%       0.000000  \n",
       "50%       0.000000  \n",
       "75%       1.000000  \n",
       "max       1.000000  \n",
       "\n",
       "[8 rows x 501 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2093 entries, 0 to 2092\n",
      "Columns: 501 entries, dim1 to is_person\n",
      "dtypes: float64(495), int64(6)\n",
      "memory usage: 8.0 MB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# (b) # Your Code goes here:\n",
    "display('Looking at first 5 rows:',trnData.head())\n",
    "display(\"Basic stats of datasets:\",trnData.describe())\n",
    "display(trnData.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(c) ***Your answer goes here:***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ========== Question 1.2 --- [8 marks] ==========\n",
    "\n",
    "Now we will prepare the testing set in a similar manner.\n",
    "\n",
    "&nbsp;&nbsp;**(a)** [Code] Load the testing dataset `Images_A_Test.csv` into a pandas dataframe: again extract the Visual Features and the `is_person` column. <br>\n",
    "&nbsp;&nbsp;**(b)** [Code] Using similar methods to [Q1.1](#question1_1) verify that the testing set is similar to the training set.<br>\n",
    "&nbsp;&nbsp;**(c)** [Text] Indicate the dimensionality, and comment on any discrepancies if any (if they are similar, just say so)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-03T15:51:56.407407Z",
     "start_time": "2018-11-03T15:51:56.059659Z"
    }
   },
   "outputs": [],
   "source": [
    "# (a) # Your Code goes here:\n",
    "\n",
    "datapath = os.path.join(os.getcwd(),'datasets','Images_A_Test.csv')\n",
    "testData = pd.read_csv(datapath, delimiter=',')\n",
    "# display(testData.head())\n",
    "classLabels = testData.columns[-19:]\n",
    "classLabels = classLabels.drop('is_person')\n",
    "# display(classLabels)\n",
    "testData.drop(axis=1, inplace=True, columns=classLabels)\n",
    "testData.drop(axis=1, inplace=True, columns='imgId')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-03T15:51:58.057488Z",
     "start_time": "2018-11-03T15:51:56.411079Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Looking at first 5 rows:'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dim1</th>\n",
       "      <th>dim2</th>\n",
       "      <th>dim3</th>\n",
       "      <th>dim4</th>\n",
       "      <th>dim5</th>\n",
       "      <th>dim6</th>\n",
       "      <th>dim7</th>\n",
       "      <th>dim8</th>\n",
       "      <th>dim9</th>\n",
       "      <th>dim10</th>\n",
       "      <th>...</th>\n",
       "      <th>dim492</th>\n",
       "      <th>dim493</th>\n",
       "      <th>dim494</th>\n",
       "      <th>dim495</th>\n",
       "      <th>dim496</th>\n",
       "      <th>dim497</th>\n",
       "      <th>dim498</th>\n",
       "      <th>dim499</th>\n",
       "      <th>dim500</th>\n",
       "      <th>is_person</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.001698</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003057</td>\n",
       "      <td>0.002378</td>\n",
       "      <td>0.001019</td>\n",
       "      <td>0.001698</td>\n",
       "      <td>0.000340</td>\n",
       "      <td>0.001019</td>\n",
       "      <td>0.001359</td>\n",
       "      <td>0.004416</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002038</td>\n",
       "      <td>0.000340</td>\n",
       "      <td>0.003397</td>\n",
       "      <td>0.004416</td>\n",
       "      <td>0.000679</td>\n",
       "      <td>0.003736</td>\n",
       "      <td>0.005774</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.002038</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004076</td>\n",
       "      <td>0.001019</td>\n",
       "      <td>0.001019</td>\n",
       "      <td>0.001019</td>\n",
       "      <td>0.000679</td>\n",
       "      <td>0.001019</td>\n",
       "      <td>0.001019</td>\n",
       "      <td>0.002378</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003736</td>\n",
       "      <td>0.003397</td>\n",
       "      <td>0.005435</td>\n",
       "      <td>0.002038</td>\n",
       "      <td>0.003397</td>\n",
       "      <td>0.001019</td>\n",
       "      <td>0.001359</td>\n",
       "      <td>0.002717</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.001116</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005208</td>\n",
       "      <td>0.001860</td>\n",
       "      <td>0.001116</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001488</td>\n",
       "      <td>0.002232</td>\n",
       "      <td>0.001116</td>\n",
       "      <td>0.000372</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.014137</td>\n",
       "      <td>0.001116</td>\n",
       "      <td>0.004836</td>\n",
       "      <td>0.001488</td>\n",
       "      <td>0.000372</td>\n",
       "      <td>0.001116</td>\n",
       "      <td>0.001488</td>\n",
       "      <td>0.004092</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.001698</td>\n",
       "      <td>0.000340</td>\n",
       "      <td>0.004076</td>\n",
       "      <td>0.000679</td>\n",
       "      <td>0.002038</td>\n",
       "      <td>0.001019</td>\n",
       "      <td>0.002038</td>\n",
       "      <td>0.001019</td>\n",
       "      <td>0.000340</td>\n",
       "      <td>0.004076</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000340</td>\n",
       "      <td>0.011889</td>\n",
       "      <td>0.001698</td>\n",
       "      <td>0.001698</td>\n",
       "      <td>0.002378</td>\n",
       "      <td>0.002378</td>\n",
       "      <td>0.002038</td>\n",
       "      <td>0.001698</td>\n",
       "      <td>0.001698</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.003736</td>\n",
       "      <td>0.001019</td>\n",
       "      <td>0.000679</td>\n",
       "      <td>0.001019</td>\n",
       "      <td>0.003736</td>\n",
       "      <td>0.002038</td>\n",
       "      <td>0.002038</td>\n",
       "      <td>0.001359</td>\n",
       "      <td>0.002717</td>\n",
       "      <td>0.001698</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000679</td>\n",
       "      <td>0.000679</td>\n",
       "      <td>0.001359</td>\n",
       "      <td>0.001019</td>\n",
       "      <td>0.001019</td>\n",
       "      <td>0.001019</td>\n",
       "      <td>0.002378</td>\n",
       "      <td>0.001359</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 501 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       dim1      dim2      dim3      dim4      dim5      dim6      dim7  \\\n",
       "0  0.001698  0.000000  0.003057  0.002378  0.001019  0.001698  0.000340   \n",
       "1  0.002038  0.000000  0.004076  0.001019  0.001019  0.001019  0.000679   \n",
       "2  0.001116  0.000000  0.005208  0.001860  0.001116  0.000000  0.001488   \n",
       "3  0.001698  0.000340  0.004076  0.000679  0.002038  0.001019  0.002038   \n",
       "4  0.003736  0.001019  0.000679  0.001019  0.003736  0.002038  0.002038   \n",
       "\n",
       "       dim8      dim9     dim10    ...        dim492    dim493    dim494  \\\n",
       "0  0.001019  0.001359  0.004416    ...      0.000000  0.002038  0.000340   \n",
       "1  0.001019  0.001019  0.002378    ...      0.000000  0.003736  0.003397   \n",
       "2  0.002232  0.001116  0.000372    ...      0.000000  0.014137  0.001116   \n",
       "3  0.001019  0.000340  0.004076    ...      0.000340  0.011889  0.001698   \n",
       "4  0.001359  0.002717  0.001698    ...      0.000679  0.000679  0.001359   \n",
       "\n",
       "     dim495    dim496    dim497    dim498    dim499    dim500  is_person  \n",
       "0  0.003397  0.004416  0.000679  0.003736  0.005774  0.007812          1  \n",
       "1  0.005435  0.002038  0.003397  0.001019  0.001359  0.002717          1  \n",
       "2  0.004836  0.001488  0.000372  0.001116  0.001488  0.004092          1  \n",
       "3  0.001698  0.002378  0.002378  0.002038  0.001698  0.001698          1  \n",
       "4  0.001019  0.001019  0.001019  0.002378  0.001359  0.000000          1  \n",
       "\n",
       "[5 rows x 501 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Basic stats of datasets:'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dim1</th>\n",
       "      <th>dim2</th>\n",
       "      <th>dim3</th>\n",
       "      <th>dim4</th>\n",
       "      <th>dim5</th>\n",
       "      <th>dim6</th>\n",
       "      <th>dim7</th>\n",
       "      <th>dim8</th>\n",
       "      <th>dim9</th>\n",
       "      <th>dim10</th>\n",
       "      <th>...</th>\n",
       "      <th>dim492</th>\n",
       "      <th>dim493</th>\n",
       "      <th>dim494</th>\n",
       "      <th>dim495</th>\n",
       "      <th>dim496</th>\n",
       "      <th>dim497</th>\n",
       "      <th>dim498</th>\n",
       "      <th>dim499</th>\n",
       "      <th>dim500</th>\n",
       "      <th>is_person</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1113.000000</td>\n",
       "      <td>1113.000000</td>\n",
       "      <td>1113.000000</td>\n",
       "      <td>1113.000000</td>\n",
       "      <td>1113.000000</td>\n",
       "      <td>1113.000000</td>\n",
       "      <td>1113.000000</td>\n",
       "      <td>1113.000000</td>\n",
       "      <td>1113.000000</td>\n",
       "      <td>1113.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1113.000000</td>\n",
       "      <td>1113.000000</td>\n",
       "      <td>1113.000000</td>\n",
       "      <td>1113.000000</td>\n",
       "      <td>1113.000000</td>\n",
       "      <td>1113.000000</td>\n",
       "      <td>1113.000000</td>\n",
       "      <td>1113.000000</td>\n",
       "      <td>1113.000000</td>\n",
       "      <td>1113.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.001744</td>\n",
       "      <td>0.000702</td>\n",
       "      <td>0.004720</td>\n",
       "      <td>0.001978</td>\n",
       "      <td>0.002321</td>\n",
       "      <td>0.002110</td>\n",
       "      <td>0.001037</td>\n",
       "      <td>0.002529</td>\n",
       "      <td>0.002006</td>\n",
       "      <td>0.002641</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000598</td>\n",
       "      <td>0.004817</td>\n",
       "      <td>0.001585</td>\n",
       "      <td>0.003611</td>\n",
       "      <td>0.002196</td>\n",
       "      <td>0.002772</td>\n",
       "      <td>0.002239</td>\n",
       "      <td>0.002407</td>\n",
       "      <td>0.003097</td>\n",
       "      <td>0.473495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.001209</td>\n",
       "      <td>0.001364</td>\n",
       "      <td>0.003876</td>\n",
       "      <td>0.001417</td>\n",
       "      <td>0.001558</td>\n",
       "      <td>0.001559</td>\n",
       "      <td>0.000885</td>\n",
       "      <td>0.002736</td>\n",
       "      <td>0.001919</td>\n",
       "      <td>0.002293</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001241</td>\n",
       "      <td>0.005831</td>\n",
       "      <td>0.001243</td>\n",
       "      <td>0.002471</td>\n",
       "      <td>0.001575</td>\n",
       "      <td>0.003182</td>\n",
       "      <td>0.001346</td>\n",
       "      <td>0.001868</td>\n",
       "      <td>0.002590</td>\n",
       "      <td>0.499521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000744</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001698</td>\n",
       "      <td>0.001019</td>\n",
       "      <td>0.001172</td>\n",
       "      <td>0.001019</td>\n",
       "      <td>0.000340</td>\n",
       "      <td>0.000679</td>\n",
       "      <td>0.000781</td>\n",
       "      <td>0.001019</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000679</td>\n",
       "      <td>0.000679</td>\n",
       "      <td>0.001860</td>\n",
       "      <td>0.001019</td>\n",
       "      <td>0.000679</td>\n",
       "      <td>0.001250</td>\n",
       "      <td>0.001019</td>\n",
       "      <td>0.001359</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.001563</td>\n",
       "      <td>0.000340</td>\n",
       "      <td>0.003736</td>\n",
       "      <td>0.001698</td>\n",
       "      <td>0.002038</td>\n",
       "      <td>0.001803</td>\n",
       "      <td>0.000781</td>\n",
       "      <td>0.001698</td>\n",
       "      <td>0.001563</td>\n",
       "      <td>0.002038</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002734</td>\n",
       "      <td>0.001359</td>\n",
       "      <td>0.003125</td>\n",
       "      <td>0.001860</td>\n",
       "      <td>0.001838</td>\n",
       "      <td>0.002038</td>\n",
       "      <td>0.002038</td>\n",
       "      <td>0.002604</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.002378</td>\n",
       "      <td>0.000758</td>\n",
       "      <td>0.007102</td>\n",
       "      <td>0.002734</td>\n",
       "      <td>0.003057</td>\n",
       "      <td>0.002976</td>\n",
       "      <td>0.001488</td>\n",
       "      <td>0.003397</td>\n",
       "      <td>0.002717</td>\n",
       "      <td>0.003397</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000679</td>\n",
       "      <td>0.006793</td>\n",
       "      <td>0.002232</td>\n",
       "      <td>0.004836</td>\n",
       "      <td>0.003057</td>\n",
       "      <td>0.003736</td>\n",
       "      <td>0.003057</td>\n",
       "      <td>0.003348</td>\n",
       "      <td>0.004092</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.007133</td>\n",
       "      <td>0.022135</td>\n",
       "      <td>0.023438</td>\n",
       "      <td>0.008929</td>\n",
       "      <td>0.010417</td>\n",
       "      <td>0.010789</td>\n",
       "      <td>0.005757</td>\n",
       "      <td>0.022396</td>\n",
       "      <td>0.025000</td>\n",
       "      <td>0.013927</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012747</td>\n",
       "      <td>0.042026</td>\n",
       "      <td>0.009821</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.013346</td>\n",
       "      <td>0.029225</td>\n",
       "      <td>0.007068</td>\n",
       "      <td>0.011889</td>\n",
       "      <td>0.026786</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 501 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              dim1         dim2         dim3         dim4         dim5  \\\n",
       "count  1113.000000  1113.000000  1113.000000  1113.000000  1113.000000   \n",
       "mean      0.001744     0.000702     0.004720     0.001978     0.002321   \n",
       "std       0.001209     0.001364     0.003876     0.001417     0.001558   \n",
       "min       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "25%       0.000744     0.000000     0.001698     0.001019     0.001172   \n",
       "50%       0.001563     0.000340     0.003736     0.001698     0.002038   \n",
       "75%       0.002378     0.000758     0.007102     0.002734     0.003057   \n",
       "max       0.007133     0.022135     0.023438     0.008929     0.010417   \n",
       "\n",
       "              dim6         dim7         dim8         dim9        dim10  \\\n",
       "count  1113.000000  1113.000000  1113.000000  1113.000000  1113.000000   \n",
       "mean      0.002110     0.001037     0.002529     0.002006     0.002641   \n",
       "std       0.001559     0.000885     0.002736     0.001919     0.002293   \n",
       "min       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "25%       0.001019     0.000340     0.000679     0.000781     0.001019   \n",
       "50%       0.001803     0.000781     0.001698     0.001563     0.002038   \n",
       "75%       0.002976     0.001488     0.003397     0.002717     0.003397   \n",
       "max       0.010789     0.005757     0.022396     0.025000     0.013927   \n",
       "\n",
       "          ...            dim492       dim493       dim494       dim495  \\\n",
       "count     ...       1113.000000  1113.000000  1113.000000  1113.000000   \n",
       "mean      ...          0.000598     0.004817     0.001585     0.003611   \n",
       "std       ...          0.001241     0.005831     0.001243     0.002471   \n",
       "min       ...          0.000000     0.000000     0.000000     0.000000   \n",
       "25%       ...          0.000000     0.000679     0.000679     0.001860   \n",
       "50%       ...          0.000000     0.002734     0.001359     0.003125   \n",
       "75%       ...          0.000679     0.006793     0.002232     0.004836   \n",
       "max       ...          0.012747     0.042026     0.009821     0.015625   \n",
       "\n",
       "            dim496       dim497       dim498       dim499       dim500  \\\n",
       "count  1113.000000  1113.000000  1113.000000  1113.000000  1113.000000   \n",
       "mean      0.002196     0.002772     0.002239     0.002407     0.003097   \n",
       "std       0.001575     0.003182     0.001346     0.001868     0.002590   \n",
       "min       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "25%       0.001019     0.000679     0.001250     0.001019     0.001359   \n",
       "50%       0.001860     0.001838     0.002038     0.002038     0.002604   \n",
       "75%       0.003057     0.003736     0.003057     0.003348     0.004092   \n",
       "max       0.013346     0.029225     0.007068     0.011889     0.026786   \n",
       "\n",
       "         is_person  \n",
       "count  1113.000000  \n",
       "mean      0.473495  \n",
       "std       0.499521  \n",
       "min       0.000000  \n",
       "25%       0.000000  \n",
       "50%       0.000000  \n",
       "75%       1.000000  \n",
       "max       1.000000  \n",
       "\n",
       "[8 rows x 501 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1113 entries, 0 to 1112\n",
      "Columns: 501 entries, dim1 to is_person\n",
      "dtypes: float64(494), int64(7)\n",
      "memory usage: 4.3 MB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# (b) # Your Code goes here:\n",
    "display('Looking at first 5 rows:',testData.head())\n",
    "display(\"Basic stats of datasets:\",testData.describe())\n",
    "display(testData.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(c) ***Your answer goes here:***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ========== Question 1.3 --- [5 marks] ==========\n",
    "\n",
    "We will now prepare the data for training.\n",
    "\n",
    "&nbsp;&nbsp;**(a)** [Code] Split both the training and testing sets into a matrix of features (independent) variables [X_tr/X_tst] and a vector of prediction (dependent) variables [y_tr/y_tst]. ***[Optional]*** *As a sanity check, you may wish to verify the dimensionality of the X/y variables*.<br>\n",
    "&nbsp;&nbsp;**(b)** [Code] Using seaborn's [countplot](https://seaborn.github.io/generated/seaborn.countplot.html?highlight=countplot#seaborn.countplot) function, visualise the distribution of the person-class (True/False) in the training and testing sets (use two figures or sub-plots). Annotate your figures.<br>\n",
    "&nbsp;&nbsp;**(c)** [Text] Do you envision any problems with the distribution under both sets? Would classification accuracy be a good metric for evaluating the performance of the classifiers? Why or why not?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-03T15:51:58.082326Z",
     "start_time": "2018-11-03T15:51:58.062515Z"
    }
   },
   "outputs": [],
   "source": [
    "# (a) # Your Code goes here:\n",
    "\n",
    "X_train = trnData.drop(columns='is_person')\n",
    "X_test = testData.drop(columns='is_person')\n",
    "y_train = trnData['is_person']\n",
    "y_test =  testData['is_person']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-03T15:51:58.524848Z",
     "start_time": "2018-11-03T15:51:58.091811Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1a1e2c0198>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 504x720 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# (b) # Your Code goes here:\n",
    "f, (ax1,ax2) = plt.subplots(2, 1)\n",
    "f.set_figheight(10)\n",
    "f.set_figwidth(7)\n",
    "(sns.countplot(y_test,ax=ax1))\n",
    "(sns.countplot(y_train,ax=ax2))\n",
    "\n",
    "# f, (ax1, ax2) = plt.subplots(1, 2, sharey=True)\n",
    "# ax1 = sns.countplot(y_train)\n",
    "# ax2 = sns.countplot(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(c) ***Your answer goes here:***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Exploring Different Models for Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ========== Question 2.1 --- [3 marks] ==========\n",
    "\n",
    "As always, we wish to start with a very simple baseline classifier, which will provide a sanity check when training more advanced models.\n",
    "\n",
    "&nbsp;&nbsp;**(a)** [Text] Define a baseline classifier (indicate why you chose it/why it is relevant).<br>\n",
    "&nbsp;&nbsp;**(b)** [Code] Report the accuracy such a classifier would achieve on the testing set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a) ***Your answer goes here:***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-03T15:53:19.599157Z",
     "start_time": "2018-11-03T15:53:19.577333Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy score using a dummy classifier set to predict the most frequent class is:  0.527\n"
     ]
    }
   ],
   "source": [
    "# (b) # Your Code goes here:\n",
    "X= X_train\n",
    "y= y_train\n",
    "dummyCl = DummyClassifier(strategy='most_frequent')\n",
    "dummyCl.fit(X,y)\n",
    "dummyCl.score(X,y)\n",
    "print(\"The accuracy score using a dummy classifier set to predict \"\n",
    "      + \"the most frequent class is: \", '%.3f'%dummyCl.score(X_test,y_test) )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ========== Question 2.2 --- [9 marks] ==========\n",
    "<a id='question2_2'></a>\n",
    "Let us now train a more advanced Model.\n",
    "\n",
    "&nbsp;&nbsp;**(a)** [Code] Train a [`LogisticRegression`](http://scikit-learn.org/0.19/modules/generated/sklearn.linear_model.LogisticRegression.html) classifier using default settings, except for the `solver` parameter which you should set to `lbfgs`. Report the classification accuracy score on the testing set.<br>\n",
    "&nbsp;&nbsp;**(b)** [Text] Comment on the performance of the Logistic Regressor in comparison with the baseline model.<br>\n",
    "&nbsp;&nbsp;**(c)** [Code] Visualise the errors using an appropriate method to justify your answer to (c).<br>\n",
    "&nbsp;&nbsp;**(d)** [Text] Referring back to the observations in [Q1.1](#question1_1), and assuming that we know that the features should be informative, why do you think this may be happening?<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-03T15:56:20.292900Z",
     "start_time": "2018-11-03T15:56:20.210525Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification accuracy on test set: 0.527\n"
     ]
    }
   ],
   "source": [
    "# (a) # Your Code goes here:\n",
    "lr = LogisticRegression(solver='lbfgs')\n",
    "lr.fit(X_train, y_train)\n",
    "# print('Classification accuracy on training set: {:.3f}'.format(lr.score(X_train, y_train)))\n",
    "print('Classification accuracy on test set: {:.3f}'.format(lr.score(X_test, y_test)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b) ***Your answer goes here:***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-03T16:01:01.773884Z",
     "start_time": "2018-11-03T16:01:01.569411Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# (c) # Your Code goes here:\n",
    "plot_confusion_matrix(cm=confusion_matrix(y_test,\n",
    "                                          lr.predict(X_test)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(d) ***Your answer goes here:***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ========== Question 2.3 --- [13 marks] ==========\n",
    "\n",
    "You should have noticed that the performance of the above logistic regressor is less than satisfactory. Let us attempt to fix this by preprocessing the inputs `X`.\n",
    "\n",
    "&nbsp;&nbsp;**(a)** [Text] Before applying the processing, comment on whether you should base any parameters of the preprocessing on the training or testing set or both and what repurcussions this may have.<br>\n",
    "&nbsp;&nbsp;**(b)** [Code] Following from your observations in [Q2.2.(e)](#question2_2), process the features in both the **training** as well as the **testing** sets accordingly. *Hint: There is an sklearn [package](http://scikit-learn.org/0.19/modules/preprocessing.html) which may be very useful.* <br>\n",
    "&nbsp;&nbsp;**(c)** [Code] Now Train a Logistic Regressor on the transformed training set, keeping the same settings as in the previous question. Report the classification accuracy on the testing set and visualise the errors in a similar way to [Q2.2(d)](#question2_2). <br>\n",
    "&nbsp;&nbsp;**(d)** [Text] Finally comment on the comparative performance with [Q2.2](#question2_2)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a) ***Your answer goes here:***  \n",
    "TODO\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-03T15:51:58.728117Z",
     "start_time": "2018-11-03T15:51:58.681639Z"
    }
   },
   "outputs": [],
   "source": [
    "# (b) # Your Code goes here:\n",
    "X_train_scaled = preprocessing.scale(X_train)\n",
    "X_test_scaled = preprocessing.scale(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-03T16:05:43.491859Z",
     "start_time": "2018-11-03T16:05:43.132506Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification accuracy on training set: 0.811\n",
      "Classification accuracy on test set: 0.642\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# (c) # Your Code goes here:\n",
    "lr = LogisticRegression(solver='lbfgs')\n",
    "lr.fit(X_train_scaled, y_train)\n",
    "print('Classification accuracy on training set: {:.3f}'.format(lr.score(X_train_scaled, y_train)))\n",
    "print('Classification accuracy on test set: {:.3f}'.format(lr.score(X_test_scaled, y_test)))\n",
    "\n",
    "plot_confusion_matrix(cm=confusion_matrix(y_test,\n",
    "                                          lr.predict(X_test_scaled)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(d) ***Your answer goes here:***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ========== Question 2.4 --- [18 marks] ==========\n",
    "<a id='question2_4'></a>\n",
    "So far we have used default settings for training the logistic regression classifier. Now we want to optimise the hyperparameters of the classifier, namely the regularisation parameter `C`. We will do this through [K-fold cross-validation](http://scikit-learn.org/0.19/modules/generated/sklearn.model_selection.KFold.html). You should familiarise yourself with the interpretation of the `C` parameter.\n",
    "\n",
    "&nbsp;&nbsp;**(a)** [Text] Why do we use cross-validation to optimise the hyper-parameters, rather than using the test-set?<br>\n",
    "&nbsp;&nbsp;**(b)** [Code] Load the datasets `Images_B_Train.csv` and `Images_B_Test.csv` (this ensures everyone is using the same pre-processed data). Again, extract the relevant columns (`dim1` through `dim500` and the `is_person` class) from each dataset, and store into `X_train`/`X_test` and `y_train`/`y_test` variables.<br>\n",
    "&nbsp;&nbsp;**(c)** [Code] Using Cross-Validation on the **Training** set (a 5-fold split should be sufficient: set `shuffle=True` and `random_state=0`), perform a search for the best value of `C` in the range `1e-5` to `1e5` (*Hint: the KFold [split](http://scikit-learn.org/0.19/modules/generated/sklearn.model_selection.KFold.html#sklearn.model_selection.KFold.split) method will come in handy*). Keep track of the validation-set accuracy per-fold for each value of `C` in an array. Think carefully about the best way to cover the search space: i.e. the step-lengths and number of steps.<br>\n",
    "&nbsp;&nbsp;**(d)** [Code] Plot the mean and standard-deviation (across folds) of the accuracy as a function of `C`. *Hint: you may find the matplotlib's [errorbar](https://matplotlib.org/2.2.3/api/_as_gen/matplotlib.pyplot.errorbar.html) function useful. Be careful to use the correct scale on the x-axis.* Using the mean values, report the regularisation parameter with the best accuracy (alongside its accuracy): *N.B. Do not pick the optimal value \"by hand\", instead use an appropriate numpy function*.<br>\n",
    "&nbsp;&nbsp;**(e)** [Text] Comment on the output, especially as regards the effect of the regularisation parameter (you should write between 3 and 4 sentences).<br>\n",
    "&nbsp;&nbsp;**(f)** [Code] By using the optimal value (i.e. the one that yields the highest average K-Fold classification accuracy) train a new `LogisticRegression` classifier and report the classification accuracy on the validation set.\n",
    "\n",
    "**N.B.: Keep track of the KFold object you created as we will keep using it**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a) ***Your answer goes here:***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-03T15:51:59.688239Z",
     "start_time": "2018-11-03T15:51:59.018165Z"
    }
   },
   "outputs": [],
   "source": [
    "# (b) # Your Code goes here:\n",
    "datapath = os.path.join(os.getcwd(),'datasets','Images_A_Train.csv')\n",
    "trnData = pd.read_csv(datapath,delimiter=',')\n",
    "# data = data.drop(axis=1)\n",
    "classLabels = trnData.columns[-19:]\n",
    "classLabels = classLabels.drop('is_person')\n",
    "# display(classLabels)\n",
    "trnData.drop(axis=1, inplace=True, columns=classLabels)\n",
    "trnData.drop(axis=1, inplace=True, columns='imgId')\n",
    "\n",
    "datapath = os.path.join(os.getcwd(),'datasets','Images_A_Test.csv')\n",
    "testData = pd.read_csv(datapath, delimiter=',')\n",
    "# display(testData.head())\n",
    "classLabels = testData.columns[-19:]\n",
    "classLabels = classLabels.drop('is_person')\n",
    "# display(classLabels)\n",
    "testData.drop(axis=1, inplace=True, columns=classLabels)\n",
    "testData.drop(axis=1, inplace=True, columns='imgId')\n",
    "\n",
    "X_train = trnData.drop(columns='is_person')\n",
    "X_test = testData.drop(columns='is_person')\n",
    "y_train = trnData['is_person']\n",
    "y_test =  testData['is_person']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-03T15:52:00.037348Z",
     "start_time": "2018-11-03T15:51:59.690610Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "TRAIN: [   0    1    2 ... 2090 2091 2092] TEST: [   6    9   10   14   17   18   27   30   31   34   37   47   48   53\n",
      "   55   57   61   70   76   77   80   85  107  117  118  124  135  141\n",
      "  145  148  152  156  161  175  182  187  191  200  202  215  220  223\n",
      "  227  229  233  240  245  249  251  252  253  254  259  260  262  264\n",
      "  267  276  279  289  294  299  302  303  310  311  314  317  333  342\n",
      "  353  359  361  376  379  384  385  386  390  391  393  399  402  405\n",
      "  425  427  436  438  440  443  446  452  453  457  458  459  461  465\n",
      "  475  478  485  486  489  491  503  505  512  517  518  521  522  526\n",
      "  530  536  538  546  553  558  563  564  565  572  575  578  582  583\n",
      "  587  596  602  609  610  621  629  634  638  641  651  667  672  674\n",
      "  687  689  692  708  711  712  713  716  721  722  726  731  733  740\n",
      "  758  759  764  776  781  793  795  796  799  801  805  811  813  820\n",
      "  825  828  829  831  838  839  840  842  875  881  884  891  896  897\n",
      "  898  905  913  914  918  921  924  927  937  948  953  955  958  962\n",
      "  985  987  988  994  995  997  999 1002 1010 1014 1017 1025 1029 1031\n",
      " 1032 1042 1044 1045 1049 1050 1055 1064 1069 1070 1073 1074 1079 1087\n",
      " 1088 1093 1096 1099 1106 1109 1110 1114 1115 1116 1124 1126 1129 1145\n",
      " 1146 1147 1148 1154 1165 1166 1170 1180 1183 1185 1187 1188 1189 1192\n",
      " 1197 1200 1205 1210 1211 1212 1216 1224 1239 1240 1254 1256 1260 1271\n",
      " 1273 1275 1279 1291 1294 1296 1299 1303 1310 1326 1327 1330 1332 1335\n",
      " 1344 1354 1358 1364 1366 1372 1380 1382 1385 1386 1390 1391 1396 1421\n",
      " 1422 1424 1427 1428 1432 1433 1444 1448 1449 1452 1459 1463 1464 1467\n",
      " 1477 1486 1489 1492 1503 1507 1508 1511 1515 1519 1524 1528 1534 1551\n",
      " 1562 1570 1571 1573 1574 1592 1610 1620 1623 1627 1628 1635 1637 1639\n",
      " 1648 1665 1669 1670 1676 1682 1687 1689 1704 1707 1711 1717 1719 1724\n",
      " 1743 1752 1753 1755 1762 1764 1766 1780 1782 1783 1797 1800 1804 1810\n",
      " 1811 1815 1818 1844 1845 1850 1872 1874 1878 1879 1890 1892 1897 1900\n",
      " 1910 1917 1919 1922 1924 1939 1941 1944 1946 1951 1955 1958 1960 1962\n",
      " 1965 1972 1975 1979 1993 1994 1998 1999 2003 2022 2029 2033 2037 2038\n",
      " 2042 2050 2051 2057 2060 2062 2068 2069 2074 2075 2082 2085 2088]\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'[   0    1    2 ... 2090 2091 2092] not in index'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-f2e0b371d18a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mtrain_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_index\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"TRAIN:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"TEST:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mX_train1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0my_train1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py3iaml/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2680\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mSeries\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIndex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2681\u001b[0m             \u001b[0;31m# either boolean or fancy integer index\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2682\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2683\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2684\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py3iaml/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_getitem_array\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2724\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_take\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2725\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2726\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convert_to_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2727\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_take\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2728\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py3iaml/lib/python3.7/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_convert_to_indexer\u001b[0;34m(self, obj, axis, is_setter)\u001b[0m\n\u001b[1;32m   1325\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m                     raise KeyError('{mask} not in index'\n\u001b[0;32m-> 1327\u001b[0;31m                                    .format(mask=objarr[mask]))\n\u001b[0m\u001b[1;32m   1328\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values_from_object\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: '[   0    1    2 ... 2090 2091 2092] not in index'"
     ]
    }
   ],
   "source": [
    "# (c) # Your Code goes here:\n",
    "\n",
    "c= np.arange(1e-5,1e5,100)\n",
    "lengthC = len(c)\n",
    "print(lengthC)\n",
    "kf = KFold(n_splits=5,shuffle=True, random_state=0)\n",
    "for i in range(lengthC): \n",
    "    for train_index, test_index in kf.split(X_train):\n",
    "        print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "        X_train1, X_test1 = X_train[train_index], X_train[test_index]\n",
    "        y_train1, y_test1 = y_train[train_index], y_train[test_index]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-03T15:52:00.038873Z",
     "start_time": "2018-11-03T15:51:52.261Z"
    }
   },
   "outputs": [],
   "source": [
    "# (d) # Your Code goes here:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(e) ***Your answer goes here:***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-03T15:52:00.040927Z",
     "start_time": "2018-11-03T15:51:52.275Z"
    }
   },
   "outputs": [],
   "source": [
    "# (f) # Your Code goes here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ========== Question 2.5 --- (LEVEL 11) --- [12 marks] ==========\n",
    "\n",
    "Let us attempt to validate the importance of the various features for classification. We could do this like we did for linear regression by looking at the magnitude of the weights. However, in this case, we will use the [`RandomForestClassifier`](http://scikit-learn.org/0.19/modules/generated/sklearn.ensemble.RandomForestClassifier.html) to give us a ranking over features.\n",
    "\n",
    "&nbsp;&nbsp;**(a)** [Text] How can we use the Random-Forest to get this kind of analysis? *Hint: look at the `feature_importances` property in the SKLearn implementation.*<br>\n",
    "&nbsp;&nbsp;**(b)** [Code] Initialise a random forest classifier and fit the model by using training data only and 500 trees (i.e. `n_estimators=500`). Set `random_state=42` to ensure reproducible results and `criterion=entropy` but leave all other parameters at their default value. Report the accuracy score on both the training and testing sets.<br>\n",
    "&nbsp;&nbsp;**(c)** [Text] Comment on the discrepancy between training and testing accuracies.<br>\n",
    "&nbsp;&nbsp;**(d)** [Code] By using the random forest model display the names of the 10 most important features (in descending order of importance).<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a) ***Your answer goes here:***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-03T15:52:00.043219Z",
     "start_time": "2018-11-03T15:51:52.280Z"
    }
   },
   "outputs": [],
   "source": [
    "# (b) # Your Code goes here:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(c) ***Your answer goes here:***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-03T15:52:00.044712Z",
     "start_time": "2018-11-03T15:51:52.287Z"
    }
   },
   "outputs": [],
   "source": [
    "# (d) # Your Code goes here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ========== Question 2.6 --- [12 marks] ==========\n",
    "\n",
    "We would like now to explore another form of classifier: the Support Vector Machine. A key decision in training SVM's is what kind of kernel to use. We will explore with three kernel types: linear, radial-basis-functions and polynomials. To get a feel for each we will first visualise typical decision boundaries for each of these variants. To do so, we have to simplify our problem to two-dimensional input (to allow us to visualise it).\n",
    "\n",
    "&nbsp;&nbsp;**(a)** [Code] Using the training set only, create a training `X` matrix with only the `dim21` and `dim51` columns. ***N.B.*** *Python (and numpy) use zero-based indexing*. Then train three distinct classifiers on this 2D data. Use a `linear` kernel for one, an `rbf` kernel for another (set `gamma='auto'`) and a second order (`degree`) polynomial kernel for the other. Set `C=1` in all cases. Using the function `plot_SVM_DecisionBoundary` from our own library (it exists under the `plotters` module), plot the decision boundary for all three classifiers.<br>\n",
    "&nbsp;&nbsp;**(b)** [Text] Explain (intuitively) the shape of the decision boundary for each classifier (i.e. comment on what aspect of the kernel gives rise to it). Use this to comment on how it relates to classification accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-03T15:52:00.046475Z",
     "start_time": "2018-11-03T15:51:52.292Z"
    }
   },
   "outputs": [],
   "source": [
    "# (a) # Your Code goes here:\n",
    "\n",
    "X = trnData.filter(['dim21','dim51'],axis=1)\n",
    "# print(X.shape,y_train.shape)\n",
    "\n",
    "svc_linear = LinearSVC(C=1.0)\n",
    "svc_linear.fit(X_train, y_train)\n",
    "\n",
    "svc_rbf = SVC(kernel='rbf',C=1.0,gamma='auto')\n",
    "svc_rbf.fit(X, y_train)\n",
    "\n",
    "svc_poly = SVC(kernel='poly', degree=2,C=1.0)\n",
    "svc_poly.fit(X, y_train)\n",
    "\n",
    "print('Linear SVC classification accuracy on training set: {:.3f}'.format(svc_linear.score(X_train, y_train)))\n",
    "print('RBF SVC classification accuracy on training set: {:.3f}'.format(svc_rbf.score(X, y_train)))\n",
    "print('Poly SVC classification accuracy on training set: {:.3f}'.format(svc_poly.score(X, y_train)))\n",
    "\n",
    "#plot_SVM_DecisionBoundary(['poly'],X,y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b) ***Your answer goes here:***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ========== Question 2.7 --- [14 marks] ==========\n",
    "Let us now explore the polynomial SVM further. We will go back to using the **FULL** dataset (i.e. the one we loaded in [Question 2.4](#question2_4)). There are two parameters we need to tune: the order of the polynomial and the regression coefficient. We will do this by way of a grid-search over parameters. To save computational time, we will use a constrained search space:\n",
    "\n",
    "&nbsp;&nbsp;**(a)** [Code] Define an appropriate search space for `C` in the range `1e-2` to `1e3` using 6-steps (think about the step-size), and for the `degree` in the range 1 through 5 inclusive (5 steps). Using the `K-fold` iterator from [Q2.5](#question2_4), optimise the values for `C` and the `degree` in the above specified range. Keep track of the mean cross-validation accuracy for each parameter combination.<br>\n",
    "&nbsp;&nbsp;**(b)** [Code] Using a seaborn heatmap, plot the fold-averaged classification accuracy for each parameter combination (label axes appropriately). Finally also report the combination of the parameters which yielded the best accuracy.<br>\n",
    "&nbsp;&nbsp;**(c)** [Code] Retrain the (polynomial-kernel) SVC using the optimal parameters found in **(b)** and report its accuracy on the **Testing** set.<br>\n",
    "&nbsp;&nbsp;**(d)** [Text] Explain the results relative to the Logistic Classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-03T15:52:00.048412Z",
     "start_time": "2018-11-03T15:51:52.301Z"
    }
   },
   "outputs": [],
   "source": [
    "# (a) # Your Code goes here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-03T15:52:00.050275Z",
     "start_time": "2018-11-03T15:51:52.310Z"
    }
   },
   "outputs": [],
   "source": [
    "# (b) # Your Code goes here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-03T15:52:00.052058Z",
     "start_time": "2018-11-03T15:51:52.317Z"
    }
   },
   "outputs": [],
   "source": [
    "# (c) # Your Code goes here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(d) ***Your answer goes here:***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ========== Question 2.8 --- (LEVEL 11) --- [10 marks] ==========\n",
    "\n",
    "Answer the followign theoretical questions:\n",
    "\n",
    "&nbsp;&nbsp;**(a)** [Text] Is a Logistic Regression Classifier equivalent to an SVM with a Linear Kernel? why or why not?<br>\n",
    "&nbsp;&nbsp;**(b)** [Text] In the previous question we optimised the `degree` and regularisation `C` simultaneously. By looking at the heatmap you plotted, can you explain the motivation behind this? That is, what would happen if we were to estimate the optimum along each dimension independently? Can you imagine a case where an independent search along each of the dimensions (known as coordinate-descent) would be guaranteed to yield the optimum result?<br>\n",
    "&nbsp;&nbsp;**(c)** [Text] Despite having a hold-out testing set, we used cross-validation for optimising the hyper-parameters (only using the testing set for comparing models). Why is this? Also, mention an advantage and a disadvantage of using cross-validation to train hyper-parameters rather than a further train/validation split."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a) ***Your answer goes here:***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b) ***Your answer goes here:***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(c) ***Your answer goes here:***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix\n",
    "## More information about visual words\n",
    "\n",
    "The Visual words used in this project are based on [Scale-invariant feature transforms (SIFT)](https://en.wikipedia.org/wiki/Scale-invariant_feature_transform). SIFT features are essentially local orientation histograms and capture the properties of small image regions. They possess attractive invariance properties which make them well suited for our task (you can read more about SIFT features in [D.Lowe, IJCV 60(2):91- 110, 2004](http://link.springer.com/article/10.1023/B:VISI.0000029664.99615.94), but the details don't matter for the purpose of this assignment). Each SIFT feature is a 128 dimensional vector. From each image many SIFT features are extracted, typically > 2500 per image (features are extracted at regular intervals using a 15 pixel grid and at 4 different scales). To obtain visual words a representative subset of all extracted SIFT features from all images is chosen and clustered with k-means using 500 centres (such use of the k-means algorithm will be discussed in detail during the lecture). These 500 cluster centres form our visual words. The representation of a single image is obtained by first assigning each SIFT feature extracted from the image to the appropriate cluster (i.e. we determine the visual word corresponding to each feature by picking the closest cluster centre). We then count the number of features from that image assigned to each cluster (i.e. we determine how often each visual word is present in the image). This results in a 500 dimensional count vector for each image (one dimension for each visual word). The normalized version of this count vector gives the final representation of the image (normalized means that we divide the count vector by the total number of visual words in the image, i.e. the normalized counts sum to 1 for each image)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
